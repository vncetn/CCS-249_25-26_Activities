{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f70ec4",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .info-card {\n",
    "        max-width: 650px;\n",
    "        margin: 25px auto;\n",
    "        padding: 25px 30px;\n",
    "        border: 1px solid #e0e0e0;\n",
    "        border-radius: 12px;\n",
    "        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.05);\n",
    "        font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif;\n",
    "        background-color: #fdfdfd;\n",
    "        color: #333;\n",
    "    }\n",
    "    .info-card .title {\n",
    "        color: #1a237e; /* Dark Indigo */\n",
    "        font-size: 24px;\n",
    "        font-weight: 600;\n",
    "        margin-top: 0;\n",
    "        margin-bottom: 15px;\n",
    "        text-align: center;\n",
    "        border-bottom: 2px solid #e8eaf6; /* Light Indigo */\n",
    "        padding-bottom: 10px;\n",
    "    }\n",
    "    .info-card .details-grid {\n",
    "        display: grid;\n",
    "        grid-template-columns: max-content 1fr;\n",
    "        gap: 12px 20px;\n",
    "        margin-top: 20px;\n",
    "        font-size: 16px;\n",
    "    }\n",
    "    .info-card .label {\n",
    "        font-weight: 600;\n",
    "        color: #555;\n",
    "        text-align: right;\n",
    "    }\n",
    "    .info-card .value {\n",
    "        font-weight: 400;\n",
    "        color: #222;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"info-card\">\n",
    "    <h2 class=\"title\">Unit 4.1 Exercise</h2>\n",
    "    <div class=\"details-grid\">\n",
    "        <div class=\"label\">Name:</div>\n",
    "        <div class=\"value\">Ethan Jed V. Carbonell & Vincent L. Corpes Jr.</div>\n",
    "        <div class=\"label\">Date:</div>\n",
    "        <div class=\"value\">February 12, 2026</div>\n",
    "        <div class=\"label\">Year & Section:</div>\n",
    "        <div class=\"value\">BSCS 3A AI</div>\n",
    "        <div></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d2b95",
   "metadata": {},
   "source": [
    "> # Manual Method for developing a Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92e93377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Data Definition ---\n",
    "documents = [\n",
    "    (\"Free money now!!!\", \"SPAM\"),\n",
    "    (\"Hi mom, how are you?\", \"HAM\"),\n",
    "    (\"Lowest price for your meds\", \"SPAM\"),\n",
    "    (\"Are we still on for dinner?\", \"HAM\"),\n",
    "    (\"Win a free iPhone today\", \"SPAM\"),\n",
    "    (\"Let's catch up tomorrow at the office\", \"HAM\"),\n",
    "    (\"Meeting at 3 PM tomorrow\", \"HAM\"),\n",
    "    (\"Get 50% off, limited time!\", \"SPAM\"),\n",
    "    (\"Team meeting in the office\", \"HAM\"),\n",
    "    (\"Click here for prizes!\", \"SPAM\"),\n",
    "    (\"Can you send the report?\", \"HAM\")\n",
    "]\n",
    "\n",
    "test_sentences = [\n",
    "    \"Limited offer, click here!\",\n",
    "    \"Meeting at 2 PM with the manager.\"\n",
    "]\n",
    "\n",
    "# Pre-processing\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8048b6b7",
   "metadata": {},
   "source": [
    "> A. Generate a Bag of Words (for word frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2fa48ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAG OF WORDS\n",
      "Total words in SPAM (N_spam): 22\n",
      "Total words in HAM (N_ham): 34\n",
      "Vocabulary size (|V|): 45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam_freq = defaultdict(int)\n",
    "ham_freq = defaultdict(int)\n",
    "vocabulary = set()\n",
    "\n",
    "n_spam = 0 # Total words in SPAM\n",
    "n_ham = 0  # Total words in HAM\n",
    "spam_docs_count = 0\n",
    "ham_docs_count = 0\n",
    "\n",
    "for doc, label in documents:\n",
    "    tokens = tokenize(doc)\n",
    "    if label == \"SPAM\":\n",
    "        spam_docs_count += 1\n",
    "        for token in tokens:\n",
    "            spam_freq[token] += 1\n",
    "            vocabulary.add(token)\n",
    "            n_spam += 1\n",
    "    else:\n",
    "        ham_docs_count += 1\n",
    "        for token in tokens:\n",
    "            ham_freq[token] += 1\n",
    "            vocabulary.add(token)\n",
    "            n_ham += 1\n",
    "\n",
    "v_size = len(vocabulary)\n",
    "\n",
    "print(\"BAG OF WORDS\")\n",
    "print(f\"Total words in SPAM (N_spam): {n_spam}\")\n",
    "print(f\"Total words in HAM (N_ham): {n_ham}\")\n",
    "print(f\"Vocabulary size (|V|): {v_size}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e3905c",
   "metadata": {},
   "source": [
    "> B. Calculate the prior for the class HAM and SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee4b7f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRIORS\n",
      "P(SPAM) = 5/11 = 0.4545\n",
      "P(HAM) = 6/11 = 0.5455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_docs = len(documents)\n",
    "p_spam = spam_docs_count / total_docs\n",
    "p_ham = ham_docs_count / total_docs\n",
    "\n",
    "print(\"PRIORS\")\n",
    "print(f\"P(SPAM) = {spam_docs_count}/{total_docs} = {p_spam:.4f}\")\n",
    "print(f\"P(HAM) = {ham_docs_count}/{total_docs} = {p_ham:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c878250",
   "metadata": {},
   "source": [
    "> C. Calculate the likelihood of the tokens in the vocabulary with respect to the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2ab21c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIKELIHOOD OF TOKENS\n",
      "P(3        | SPAM) = 0.0149   |   P(3        | HAM) = 0.0253\n",
      "P(50       | SPAM) = 0.0299   |   P(50       | HAM) = 0.0127\n",
      "P(a        | SPAM) = 0.0299   |   P(a        | HAM) = 0.0127\n",
      "P(are      | SPAM) = 0.0149   |   P(are      | HAM) = 0.0380\n",
      "P(at       | SPAM) = 0.0149   |   P(at       | HAM) = 0.0380\n",
      "P(can      | SPAM) = 0.0149   |   P(can      | HAM) = 0.0253\n",
      "P(catch    | SPAM) = 0.0149   |   P(catch    | HAM) = 0.0253\n",
      "P(click    | SPAM) = 0.0299   |   P(click    | HAM) = 0.0127\n",
      "P(dinner   | SPAM) = 0.0149   |   P(dinner   | HAM) = 0.0253\n",
      "P(for      | SPAM) = 0.0448   |   P(for      | HAM) = 0.0253\n",
      "P(free     | SPAM) = 0.0448   |   P(free     | HAM) = 0.0127\n",
      "P(get      | SPAM) = 0.0299   |   P(get      | HAM) = 0.0127\n",
      "P(here     | SPAM) = 0.0299   |   P(here     | HAM) = 0.0127\n",
      "P(hi       | SPAM) = 0.0149   |   P(hi       | HAM) = 0.0253\n",
      "P(how      | SPAM) = 0.0149   |   P(how      | HAM) = 0.0253\n",
      "P(in       | SPAM) = 0.0149   |   P(in       | HAM) = 0.0253\n",
      "P(iphone   | SPAM) = 0.0299   |   P(iphone   | HAM) = 0.0127\n",
      "P(let      | SPAM) = 0.0149   |   P(let      | HAM) = 0.0253\n",
      "P(limited  | SPAM) = 0.0299   |   P(limited  | HAM) = 0.0127\n",
      "P(lowest   | SPAM) = 0.0299   |   P(lowest   | HAM) = 0.0127\n",
      "P(meds     | SPAM) = 0.0299   |   P(meds     | HAM) = 0.0127\n",
      "P(meeting  | SPAM) = 0.0149   |   P(meeting  | HAM) = 0.0380\n",
      "P(mom      | SPAM) = 0.0149   |   P(mom      | HAM) = 0.0253\n",
      "P(money    | SPAM) = 0.0299   |   P(money    | HAM) = 0.0127\n",
      "P(now      | SPAM) = 0.0299   |   P(now      | HAM) = 0.0127\n",
      "P(off      | SPAM) = 0.0299   |   P(off      | HAM) = 0.0127\n",
      "P(office   | SPAM) = 0.0149   |   P(office   | HAM) = 0.0380\n",
      "P(on       | SPAM) = 0.0149   |   P(on       | HAM) = 0.0253\n",
      "P(pm       | SPAM) = 0.0149   |   P(pm       | HAM) = 0.0253\n",
      "P(price    | SPAM) = 0.0299   |   P(price    | HAM) = 0.0127\n",
      "P(prizes   | SPAM) = 0.0299   |   P(prizes   | HAM) = 0.0127\n",
      "P(report   | SPAM) = 0.0149   |   P(report   | HAM) = 0.0253\n",
      "P(s        | SPAM) = 0.0149   |   P(s        | HAM) = 0.0253\n",
      "P(send     | SPAM) = 0.0149   |   P(send     | HAM) = 0.0253\n",
      "P(still    | SPAM) = 0.0149   |   P(still    | HAM) = 0.0253\n",
      "P(team     | SPAM) = 0.0149   |   P(team     | HAM) = 0.0253\n",
      "P(the      | SPAM) = 0.0149   |   P(the      | HAM) = 0.0506\n",
      "P(time     | SPAM) = 0.0299   |   P(time     | HAM) = 0.0127\n",
      "P(today    | SPAM) = 0.0299   |   P(today    | HAM) = 0.0127\n",
      "P(tomorrow | SPAM) = 0.0149   |   P(tomorrow | HAM) = 0.0380\n",
      "P(up       | SPAM) = 0.0149   |   P(up       | HAM) = 0.0253\n",
      "P(we       | SPAM) = 0.0149   |   P(we       | HAM) = 0.0253\n",
      "P(win      | SPAM) = 0.0299   |   P(win      | HAM) = 0.0127\n",
      "P(you      | SPAM) = 0.0149   |   P(you      | HAM) = 0.0380\n",
      "P(your     | SPAM) = 0.0299   |   P(your     | HAM) = 0.0127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_likelihood(word, label):\n",
    "    \"\"\"Calculates P(word | label) using Laplace (add-1) smoothing.\"\"\"\n",
    "    if label == \"SPAM\":\n",
    "        count = spam_freq.get(word, 0)\n",
    "        # (Count + 1) / (N_spam + |V|)\n",
    "        return (count + 1) / (n_spam + v_size)\n",
    "    else:\n",
    "        count = ham_freq.get(word, 0)\n",
    "        # (Count + 1) / (N_ham + |V|)\n",
    "        return (count + 1) / (n_ham + v_size)\n",
    "print(\"LIKELIHOOD OF TOKENS\")\n",
    "for word in sorted(vocabulary):\n",
    "    prob_spam = get_likelihood(word, \"SPAM\")\n",
    "    prob_ham = get_likelihood(word, \"HAM\")\n",
    "    print(f\"P({word:<8} | SPAM) = {prob_spam:.4f}   |   P({word:<8} | HAM) = {prob_ham:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d0fc3",
   "metadata": {},
   "source": [
    "> D. Determine the class of the following test sentence:\n",
    "> > I. Limited offer, click here!\n",
    ">\n",
    "> > II. Meeting at 2 PM with the manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3becc020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SENTENCES CLASSIFICATION\n",
      "\n",
      "Test Sentence 1: 'Limited offer, click here!'\n",
      "Tokens in Vocabulary (V): ['limited', 'click', 'here']\n",
      "Score(SPAM): 12.090462 × 10^-6\n",
      "Score(HAM):  1.106311 × 10^-6\n",
      "Predicted Class: SPAM\n",
      "\n",
      "Test Sentence 2: 'Meeting at 2 PM with the manager.'\n",
      "Tokens in Vocabulary (V): ['meeting', 'at', 'pm', 'the']\n",
      "Score(SPAM): 0.022557 × 10^-6\n",
      "Score(HAM):  1.008284 × 10^-6\n",
      "Predicted Class: HAM\n"
     ]
    }
   ],
   "source": [
    "print(\"TEST SENTENCES CLASSIFICATION\")\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"\\nTest Sentence {i+1}: '{sentence}'\")\n",
    "    \n",
    "    tokens = tokenize(sentence)\n",
    "    \n",
    "    # Remove words not mentioned\n",
    "    valid_tokens = [t for t in tokens if t in vocabulary]\n",
    "    print(f\"Tokens in Vocabulary (V): {valid_tokens}\")\n",
    "    \n",
    "    # Initialize scores with the prior probabilities\n",
    "    score_spam = p_spam\n",
    "    score_ham = p_ham\n",
    "    \n",
    "    # Multiply by the likelihood of each valid token\n",
    "    for token in valid_tokens:\n",
    "        score_spam *= get_likelihood(token, \"SPAM\")\n",
    "        score_ham *= get_likelihood(token, \"HAM\")\n",
    "        \n",
    "    print(f\"Score(SPAM): {score_spam / 1e-6:.6f} × 10^-6\")\n",
    "    print(f\"Score(HAM):  {score_ham / 1e-6:.6f} × 10^-6\")\n",
    "\n",
    "    \n",
    "    # Compare scores to determine the class\n",
    "    if score_spam > score_ham:\n",
    "        print(\"Predicted Class: SPAM\")\n",
    "    else:\n",
    "        print(\"Predicted Class: HAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6976996",
   "metadata": {},
   "source": [
    "> # Using Scikit for developing a Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d32997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b53d2",
   "metadata": {},
   "source": [
    "> Defining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b85b0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Free money now!!!\",\n",
    "    \"Hi mom, how are you?\",\n",
    "    \"Lowest price for your meds\",\n",
    "    \"Are we still on for dinner?\",\n",
    "    \"Win a free iPhone today\",\n",
    "    \"Let's catch up tomorrow at the office\",\n",
    "    \"Meeting at 3 PM tomorrow\",\n",
    "    \"Get 50% off, limited time!\",\n",
    "    \"Team meeting in the office\",\n",
    "    \"Click here for prizes!\",\n",
    "    \"Can you send the report?\"\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"SPAM\", \"HAM\", \"SPAM\", \"HAM\", \"SPAM\",\n",
    "    \"HAM\", \"HAM\", \"SPAM\", \"HAM\", \"SPAM\", \"HAM\"\n",
    "]\n",
    "\n",
    "test_sentences = [\n",
    "    \"Limited offer, click here!\",\n",
    "    \"Meeting at 2 PM with the manager.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b588be",
   "metadata": {},
   "source": [
    "> Vectorizing Text Data (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3430883d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULARY LEARNED\n",
      "['3' '50' 'a' 'are' 'at' 'can' 'catch' 'click' 'dinner' 'for' 'free' 'get'\n",
      " 'here' 'hi' 'how' 'in' 'iphone' 'let' 'limited' 'lowest' 'meds' 'meeting'\n",
      " 'mom' 'money' 'now' 'off' 'office' 'on' 'pm' 'price' 'prizes' 'report'\n",
      " 's' 'send' 'still' 'team' 'the' 'time' 'today' 'tomorrow' 'up' 'we' 'win'\n",
      " 'you' 'your']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X_train = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"VOCABULARY LEARNED\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae3d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the Multinomial Naïve Bayes classifier\n",
    "# alpha=1.0 applies Laplace smoothing by default\n",
    "clf = MultinomialNB(alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234fc78c",
   "metadata": {},
   "source": [
    "> Predicting the test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f37d8928",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test_sentences)\n",
    "predictions = clf.predict(X_test)\n",
    "probabilities = clf.predict_proba(X_test)\n",
    "classes = clf.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338fa7d0",
   "metadata": {},
   "source": [
    "> Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9251c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCIKIT-LEARN PREDICTION RESULTS\n",
      "\n",
      "Test Sentence 1: 'Limited offer, click here!'\n",
      "Probability Distribution:\n",
      "P(HAM): 0.08383194\n",
      "P(SPAM): 0.91616806\n",
      "Predicted Class: SPAM\n",
      "\n",
      "Test Sentence 2: 'Meeting at 2 PM with the manager.'\n",
      "Probability Distribution:\n",
      "P(HAM): 0.97811802\n",
      "P(SPAM): 0.02188198\n",
      "Predicted Class: HAM\n"
     ]
    }
   ],
   "source": [
    "print(\"SCIKIT-LEARN PREDICTION RESULTS\")\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"\\nTest Sentence {i+1}: '{sentence}'\")\n",
    "    print(\"Probability Distribution:\")\n",
    "    for j, c in enumerate(classes):\n",
    "        print(f\"P({c}): {probabilities[i][j]:.8f}\")\n",
    "    print(f\"Predicted Class: {predictions[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
